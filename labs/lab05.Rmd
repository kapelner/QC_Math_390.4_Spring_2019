---
title: "Lab 5"
author: "Your Name Here"
output: pdf_document
date: "11:59PM March 16, 2019"
---

Load the Boston housing data frame and create the vector $y$ (the median value) and matrix $X$ (all other features) from the data frame. Name the columns the same as Boston except for the first name it "(Intercept)".

```{r}
#TO-DO
```

Run the OLS linear model to get $b$, the vector of coefficients. Do not use `lm`.

```{r}
#TO-DO
```

Find the hat matrix for this regression `H` and find its rank. Is this rank expected?

```{r}
#TO-DO
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library's `expect_equal(matrix1, matrix2, tolerance = 1e-2)`.

```{r}
#TO-DO
```

Find the matrix that projects onto the space of residuals `H_comp` and find its rank. Is this rank expected?

```{r}
#TO-DO
```

Verify this is a projection matrix by verifying the two sufficient conditions. Use the `testthat` library.

```{r}
#TO-DO
```

Calculate $\hat{y}$.

```{r}
#TO-DO
```

Calculate $e$ as the difference of $y$ and $\hat{y}$ and the projection onto the space of the residuals. Verify the two means of calculating the residuals provide the same results.

```{r}
#TO-DO
```

Calculate $R^2$ and RMSE.

```{r}
#TO-DO
```

Verify $\hat{y}$ and $e$ are orthogonal.

```{r}
#TO-DO
```

Verify $\hat{y} - \bar{y}$ and $e$ are orthogonal.

```{r}
#TO-DO
```

Find the cosine-squared of $y - \bar{y}$ and $\hat{y} - \bar{y}$ and verify it is the same as $R^2$.

```{r}
#TO-DO
```

Verify the sum of squares identity which we learned was due to the Pythagorean Theorem (applies since the projection is specifically orthogonal).

```{r}
#TO-DO
```

Create a matrix that is $(p + 1) \times (p + 1)$ full of NA's. Label the columns the same columns as X. Do not label the rows. For the first row, find the OLS estimate of the $y$ regressed on the first column only and put that in the first entry. For the second row, find the OLS estimates of the $y$ regressed on the first and second columns of $X$ only and put them in the first and second entries. For the third row, find the OLS estimates of the $y$ regressed on the first, second and third columns of $X$ only and put them in the first, second and third entries, etc. For the last row, fill it with the full OLS estimates.

```{r}
#TO-DO
```

Examine this matrix. Why are the estimates changing from row to row as you add in more predictors?

#TO-DO

Clear the workspace and load the diamonds dataset.

```{r}
#TO-DO
```

Extract $y$, the price variable and "c", the nominal variable "color" as vectors.

```{r}
#TO-DO
```

Convert the "c" vector to $X$ which contains an intercept and an appropriate number of dummies. Let the color G be the refernce category as it is the modal color. Name the columns of $X$ appropriately. The first should be "(Intercept)". Delete c.

```{r}
#TO-DO
```

Repeat the iterative exercise above we did for Boston here.

```{r}
#TO-DO
```

Why didn't the estimates change as we added more and more features?

#TO-DO


```{r}
#TO-DO
```

Create a vector $y$ by simulating $n = 100$ standard iid normals. Create a matrix of size 100 x 2 and populate the first column by all ones (for the intercept) and the second column by 100 standard iid normals. Find the $R^2$ of an OLS regression of `y ~ X`. Use matrix algebra.

```{r}
#TO-DO
```

 from the last problem. Find the $R^2$ of an OLS regression of `y ~ X`. You can use the `summary` function of an `lm` model.

Write a for loop to each time bind a new column of 100 standard iid normals to the matrix $X$ and find the $R^2$ each time until the number of columns is 100. Create a vector to save all $R^2$'s. What happened??


```{r}
#TO-DO
```

Add one final column to $X$ to bring the number of columns to 101. Then try to compute $R^2$. What happens and why?

```{r}
#TO-DO
```

