---
title: "Practice Lecture 11 MATH 390.4 Queens College"
author: "Professor Adam Kapelner"
date: "April 12, 2019"
---


# Regression Trees

Let's take a look at the simulated sine curve data (i.e. the illustration I drew on the board last class)

```{r}
pacman::p_load(ggplot2)
N = 500
x_max = 10
x = runif(N, 0, x_max)
y = sin(x) + rnorm(N, 0, 0.3)
ggplot(data.frame(x = x, y = y), aes(x, y)) + geom_point(lwd=0.6) 
```


Let's fit a regression tree. First load the package:

```{r}
options(java.parameters = "-Xmx4000m")
# pacman::p_install_gh("kapelner/YARF", subdir = "YARF", ref = "dev")
pacman::p_load(YARF)
```

Then run the fit:

```{r}
tree_mod = YARFCART(data.frame(x = x), y)
```

How "big" is this tree model?

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

What are the "main" splits?

```{r}
illustrate_trees(tree_mod, max_depth = 4, open_file = TRUE)
```

What does $g(x)$ look like?

```{r}
Nres = 1000
x_predict = data.frame(x = seq(0, x_max, length.out = Nres))
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Obviously overfit - but not that bad... let's try lowering the complexity by stopping the tree construction at a higher node size.

```{r}
tree_mod = YARFCART(data.frame(x = x), y, nodesize = 500)
g = predict(tree_mod, x_predict)
ggplot(data.frame(x = x, y = y), aes(x, y)) + 
  geom_point(lwd = 0.6) +
  geom_point(aes(x, y), data.frame(x = x_predict, y = g), col = "blue")
```

Less overfitting now. Can we try to underfit? That's easy...

Now let's look at some real data. We load the Boston Housing data:

```{r}
pacman::p_load(MASS)
data(Boston)
?Boston
```

Now we split the data:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(Boston), round((1 - test_prop) * nrow(Boston)))
Boston_train = Boston[train_indices, ]
y_train = Boston_train$medv
X_train = Boston_train
X_train$medv = NULL
```

And fit a tree model:

```{r}
tree_mod = YARFCART(X_train, y_train)
```

What does the in-sample fit look like?

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

Recall the linear model:

```{r}
linear_mod = lm(medv ~ ., Boston_train)
summary(linear_mod)$sigma
summary(linear_mod)$r.squared
```

The tree seems to win in-sample. Why? 

Is this a "fair" comparison?

Before we address this, let's illustrate the tree. 

```{r}
illustrate_trees(tree_mod, max_depth = 3, open_file = TRUE)
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

Let's make the comparison fair by seeing what happens oos.

```{r}
test_indices = setdiff(1 : nrow(Boston), train_indices)
Boston_test = Boston[test_indices, ]
y_test = Boston_test$medv
X_test = Boston_test
X_test$medv = NULL
```

For the tree:

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

For the linear model:

```{r}
y_hat_test_linear = predict(linear_mod, Boston_test)
e = y_test - y_hat_test_linear
sd(e)
1 - sd(e) / sd(y_test)
```

Let's do that over and over to get a sense of the variability. Do you see why people do cross validation / bootstrap validation? To reduce the error of the estimate. 

The take-home message here is that the tree beats the linear model in future predictive performance. Why is this?

Let's see what's going on in the tree.

```{r}
get_tree_num_nodes_leaves_max_depths(tree_mod)
```

About how many observations are in each leaf?

```{r}
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

That's a very flexible model.

Let's see overfitting in action. Let's set nodesize to be one.

```{r}
tree_mod = YARFCART(X_train, y_train, nodesize = 1)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(Boston_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

This is essentially giving each obsevation it's own guess. Just like linear modeling when $n = p + 1$ and nearest neighbors when $K = 1$. Let's see how bad the overfitting is:

```{r}
y_hat_train = predict(tree_mod, Boston_train)
e = y_train - y_hat_train
sd(e)
1 - sd(e) / sd(y_train)
```

```{r}
y_hat_test_tree = predict(tree_mod, Boston_test)
e = y_test - y_hat_test_tree
sd(e)
1 - sd(e) / sd(y_test)
```

This is strange. The overfit model still beats the linear model. It must be that the linear model is so *underfit* that it's beaten by a very *overfit* model. We can run this many times to see the variability...

```{r}
rm(list = ls())
```



# Classification Trees

Let's get the cancer biopsy data:

```{r}
biopsy = na.omit(MASS::biopsy)
biopsy$ID = NULL
str(biopsy)
?biopsy
```

Let's do a training-test split to keep things honest:

```{r}
test_prop = 0.1
train_indices = sample(1 : nrow(biopsy), round((1 - test_prop) * nrow(biopsy)))
biopsy_train = biopsy[train_indices, ]
y_train = biopsy_train$class
X_train = biopsy_train
X_train$class = NULL
test_indices = setdiff(1 : nrow(biopsy), train_indices)
biopsy_test = biopsy[test_indices, ]
y_test = biopsy_test$class
X_test = biopsy_test
X_test$class = NULL
```

Let's fit a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(biopsy_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
```

Why would the average observations per node be larger than the nodesize which is 1?

```{r}
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

How are we doing in-sample?

```{r}
y_hat_train = predict(tree_mod, biopsy_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, biopsy_test)
mean(y_test != y_hat_test)
```

Still pretty good!

Now let's take a look at the linear SVM model.

```{r}
pacman::p_load(e1071)
svm_model = svm(X_train, y_train, kernel = "linear")
```

A couple of points:

* Reached max iterations to minimize the hinge loss. Seems like there are computational issues here.
* Note that we are relying on the $\lambda$ hyperparameter value for the hinge loss. On the homework, you will answer the question we never answered: how should the value of the hyperparameter be chosen?

Regardless, how did it do in-sample?

```{r}
y_hat_train = predict(svm_model, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, X_test)
mean(y_test != y_hat_test)
```

Maybe the model truly was linearly separable? Meaning, you don't get any added benefit from the tree if there are no interactions or non-linearities. Let's try a harder dataset. First, get a bunch of datasets from the UCI repository:

```{r}
rm(list = ls())
pacman::p_load_gh("coatless/ucidata")
data(adult)
adult = na.omit(adult) #kill any observations with missingness
str(adult)
?adult
```

Let's use samples of 2,000 to run experiments:

```{r}
test_size = 2000
train_indices = sample(1 : nrow(adult), test_size)
adult_train = adult[train_indices, ]
y_train = adult_train$income
X_train = adult_train
X_train$income = NULL
test_indices = sample(setdiff(1 : nrow(adult), train_indices), test_size)
adult_test = adult[test_indices, ]
y_test = adult_test$income
X_test = adult_test
X_test$income = NULL
```

Make a tree:

```{r}
tree_mod = YARFCART(X_train, y_train)
get_tree_num_nodes_leaves_max_depths(tree_mod)
nrow(adult_train) / get_tree_num_nodes_leaves_max_depths(tree_mod)$num_leaves
illustrate_trees(tree_mod, max_depth = 5, open_file = TRUE)
```

In-sample?

```{r}
y_hat_train = predict(tree_mod, X_train)
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(tree_mod, X_test)
mean(y_test != y_hat_test)
```

Looks like we overfit quite a bit! That's what nodesize of 1 does! Why is it the default? People found that even though it overfits, you still get good performance.

Let's see how the linear SVM does:

```{r}
svm_model = svm(model.matrix(~ ., X_train), y_train, kernel = "linear")
```

In-sample?

```{r}
y_hat_train = predict(svm_model, model.matrix(~ ., X_train))
mean(y_train != y_hat_train)
```

Out of sample?

```{r}
y_hat_test = predict(svm_model, model.matrix(~ ., X_test))
mean(y_test != y_hat_test)
```

It seems (at least when I ran it at home), the linear SVM does much worse. Likely there are a lot of interactions in this dataset that the linear SVM must ignore because it's $\mathcal{H}$ candidate set is so limited!

Note: SVM train error is approximtely = SVM test error? Why? 

That's a usual scenario during underfitting in high n situations. There is no estimation error - only misspecification error and error due to ignorance.